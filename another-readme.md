# Multimodal AI Healthcare Assistant

> Voice + Vision + LLM — a demo assistant that understands **speech**, **medical images**, and **text**, then replies with **doctor‑style spoken answers**.

---

## 1) What this project is

An end‑to‑end, **patient–doctor interaction prototype** built with **Gradio**. It lets a user:

- **Speak a symptom/question** → the app records your voice and transcribes it using **Groq Whisper (`whisper-large-v3-turbo`)**.  
- **Upload a clinical image** (e.g., a skin lesion scan, X‑ray, etc.) → the app encodes it and sends it along with your query to a **multimodal LLM** running on **Groq**.  
- **Receive a doctor's reply** → generated by a **Meta Llama family** model (code references a `"meta-llama/llama-4-maverick-17b-...-turbo"` string), and then converted to **speech** using **ElevenLabs** (fallback: **gTTS**).

> ⚠️ **Medical disclaimer:** This is a **technical demo** for learning and experimentation. It is **not** a medical device and must **not** be used for diagnosis or treatment. Always consult a licensed clinician.

---
