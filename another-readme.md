# Multimodal AI Healthcare Assistant

> ðŸŽ™ï¸ **Voice** + ðŸ‘ï¸ **Vision** + ðŸ§  **LLM** â€” A demo assistant that understands **speech**, **medical images**, and **text**, then replies with **doctor-style spoken answers**.

---

## 1) ðŸŒŸ Project Introduction

This is an end-to-end **patient-doctor interaction prototype** built with **Gradio**. It allows a user to:

- ðŸ—£ï¸ **Speak a symptom/question:** The app records your voice and transcribes it using **Groq Whisper (`whisper-large-v3-turbo`)**.
- ðŸ–¼ï¸ **Upload a clinical image:** (e.g., a skin lesion scan, X-ray, etc.) â†’ The app encodes it and sends it along with your query to a **multimodal LLM** running on **Groq**.
- ðŸ’¬ **Receive a doctor's reply:** Generated by a **Meta Llama family** model, and then converted to **speech** using **ElevenLabs** (with **gTTS** as a fallback).

> âš ï¸ **Medical disclaimer:** This is a **technical demo** for learning and experimentation. It is **not** a medical device and must **not** be used for diagnosis or treatment. Always consult a licensed clinician.
---

## 2) âœ¨ Key features (A â†’ Z)

-  âœ…**A**udio input via microphone (PortAudio/PyAudio).  
-  âœ…**B**ase64 image encoding for safe multimodal requests.  
-  âœ…**C**hatâ€‘style reasoning over **text + image** with a Groqâ€‘hosted LLM.  
-  âœ…**D**eviceâ€‘agnostic web UI using **Gradio**.  
-  âœ…**E**levenLabs TTS for natural doctorâ€‘like voice; **gTTS** as a fallback.  
-  âœ…**F**Fmpeg utilities for audio processing.  
-  âœ…**G**roq **Whisper** ASR for robust transcription (`whisper-large-v3-turbo`).  
-  âœ…**H**ardening suggestions: .env secrets, input size checks, PII redaction.  
-  âœ…**I**mage understanding through a **visionâ€‘capable** chat model (multimodal).  
-  âœ…**J**ustâ€‘works local run: `python gradio_app.py`.  
-  âœ…**K**eyâ€‘based config via `.env` (`GROQ_API_KEY`, `ELEVENLABS_API_KEY`).  
-  âœ…**R**AG hooks present in code imports (future: plug a vector DB).  
-  âœ…**T**esting stubs and portability (pip/conda/pipenv).

---

---

## 3) âš™ï¸ How it works (Pipeline)

```
[Mic Input] --(wav/mp3)--> [ASR: Groq Whisper] --(text)--> 
     +--> [Optional Image Upload] --(base64)--> 
            [Multimodal LLM (Groq, Meta Llama family)] --(doctor reply)--> 
                [TTS: ElevenLabs/gTTS] --(audio)--> [Playback in Gradio]
```

Core modules (from your source code):

- `voice_of_the_patient.py`  
  - `record_audio(...)` â€“ records mic audio and writes MP3 (uses `speech_recognition`, PortAudio, FFmpeg/pydub).  
  - `transcribe_with_groq(...)` â€“ sends audio to **Groq Whisper `whisper-large-v3-turbo`** and returns text.

- `brain_of_the_doctor.py`  
  - `encode_image(path)` â€“ base64 encodes uploaded image.  
  - `analyze_image_with_query(image_path, query, model=...)` â€“ calls Groq **multimodal chat** with both the image and your question. Code string mentions **`"meta-llama/llama-4-maverick-17b-...-turbo"`**.

- `voice_of_the_doctor.py`  
  - `text_to_speech_with_elevenlabs(text, outpath)` â€“ streams highâ€‘quality TTS via **ElevenLabs**.  
  - `text_to_speech_with_gtts(text, outpath)` â€“ simple **gTTS** fallback (no API key needed).

- `gradio_app.py`  
  - Defines a **Gradio Interface** with:
    - Inputs: microphone **recording** button + **image** upload.  
    - Outputs: **transcript text**, **doctor reply**, **audio response**.  
  - Calls the three modules above to wire the full roundâ€‘trip.

---

---
## 4) ðŸ“‚ Project structure (excerpt)

```
Multimodal-AI-Healthcare-Assistant-main/
â”œâ”€â”€ gradio_app.py
â”œâ”€â”€ brain_of_the_doctor.py
â”œâ”€â”€ voice_of_the_patient.py
â”œâ”€â”€ voice_of_the_doctor.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Pipfile
â”œâ”€â”€ .env               # holds GROQ_API_KEY / ELEVENLABS_API_KEY (do NOT commit real keys)
â””â”€â”€ ...                # assets, notebooks, helpers (see repo for full tree)
```

> Autoâ€‘detected: **Gradio** app present; model artifacts folder not committed. Requirements pin **gradio**, **fastapi**, **groq**, **elevenlabs**, **speechrecognition**, **uvicorn**, etc.

---

---
## 5) ðŸš€Setup Instructions

### Prerequisites
- **Python**: recommended **3.13** (per `Pipfile`), 3.10+ also works for Gradio.  
- **FFmpeg** and **PortAudio** installed on your system.  
- Microphone permission enabled.

### Install (choose one)

**A) Pip (recommended)**
```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
```

**B) Pipenv**
```bash
pip install pipenv
pipenv install
pipenv shell
```

**C) Conda**
```bash
conda create -n multimodal-doc python=3.11 -y
conda activate multimodal-doc
pip install -r requirements.txt
```

### Environment variables
Create a `.env` in the project root:
```
GROQ_API_KEY=your_groq_key_here
ELEVENLABS_API_KEY=your_elevenlabs_key_here   # optional; if omitted, falls back to gTTS
```

---

## 6)  â–¶ï¸ How to Run

```bash
python gradio_app.py
# then open the local URL that Gradio prints (e.g., http://127.0.0.1:7860)
```

**What to try:**
1. Click **Record** and ask a medicalâ€‘style question (e.g., â€œI have a sore throat and fever for 3 days, what could it be?â€).  
2. (Optional) Upload a relevant **image** (e.g., skin lesion).  
3. Receive: transcript â†’ doctor reply (text) â†’ **audio** playback.

---


---


## 7) ðŸ› ï¸ Configuration & Customization

- **Switch TTS provider**: set `ELEVENLABS_API_KEY` to enable premium voice; remove it to use gTTS fallback.  
- **Change voices**: replace the ElevenLabs `voice_id` in `voice_of_the_doctor.py`.  
- **Model choice**: in `brain_of_the_doctor.py`, adjust the `model=` string to pick another **Groq** multimodal model.  
- **Rateâ€‘limiting & timeouts**: add guards in the Gradio handlers for production robustness.  
- **RAG**: hook a vector DB (FAISS/Chroma) and augment prompts with guideline passages.

---

---
## 8) ðŸ“Š Results & Quality Notes

This app is **interactive** rather than a fixed benchmark. For meaningful evaluation, log test sessions and report:

- **ASR quality** (WER/CER) across accents/noise.  
- **Clinical helpfulness** via curated prompts (Likert scoring by SMEs).  
- **Hallucination** rate and refusal behavior for outâ€‘ofâ€‘scope queries.  
- **Latency** (ASR â†’ LLM â†’ TTS roundâ€‘trip).

 
---

---
## 9) ðŸ¥ Realâ€‘life applications (nonâ€‘diagnostic)

- **Triage assistant** for patient education before a clinic visit.  
- **Documentation helper** to summarize patient concerns in simple language.  
- **Training/teaching** tool to discuss differentials given a symptom + image.  
- **Accessibility** aid for users who prefer speaking & listening over typing/reading.

> Always place a **medical disclaimer** in the UI and require human review.

---

---
## 10)  10. ðŸ†˜ Troubleshooting
 
- **FFmpeg/PortAudio missing** â†’ install them systemâ€‘wide; on Windows ensure `ffmpeg/bin` is in `%PATH%`.  
- **Mic not detected** â†’ check OS permissions and default input device.  
- **No speech output** â†’ verify `ELEVENLABS_API_KEY`; otherwise fallback to gTTS (internet required).  
- **Groq errors** â†’ ensure `GROQ_API_KEY` is valid; check model name and rate limits.  
- **Unicode on Windows** â†’ run in UTFâ€‘8 console (`chcp 65001`).

---

---
## 11) 11. ðŸ’» Tech Stack

- UI: **Gradio**  
- ASR: **Groq Whisper (`whisper-large-v3-turbo`)**  
- LLM (multimodal): **Meta Llama family** via **Groq** (code references a `"llama-4-maverick-17b-...-turbo"` string)  
- TTS: **ElevenLabs** (primary), **gTTS** (fallback)  
- Utilities: **pydub**, **ffmpeg**, **speech_recognition**, **uvicorn/fastapi** available for API extension

---

---

## 12)  ðŸ”’ Security & Compliance

- Store keys only in `.env` / secret managers. **Never commit real keys.**  
- If handling clinical data/images, ensure **deâ€‘identification**, consent, and compliance with local laws.  
- Add a log redactor; avoid storing raw audio/images in production.

---

---

## 13) ðŸ“„ License & Credits

- License: see repository (no explicit license file detected at generation time).  
- Credits: Groq, ElevenLabs, Gradio, and the broader openâ€‘source community.

---
