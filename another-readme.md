# Multimodal AI Healthcare Assistant

> Voice + Vision + LLM — a demo assistant that understands **speech**, **medical images**, and **text**, then replies with **doctor‑style spoken answers**.

---

## 1) What this project is ?

An end‑to‑end, **patient–doctor interaction prototype** built with **Gradio**. It lets a user:

- **Speak a symptom/question** → the app records your voice and transcribes it using **Groq Whisper (`whisper-large-v3-turbo`)**.  
- **Upload a clinical image** (e.g., a skin lesion scan, X‑ray, etc.) → the app encodes it and sends it along with your query to a **multimodal LLM** running on **Groq**.  
- **Receive a doctor's reply** → generated by a **Meta Llama family** model (code references a `"meta-llama/llama-4-maverick-17b-...-turbo"` string), and then converted to **speech** using **ElevenLabs** (fallback: **gTTS**).

> ⚠️ **Medical disclaimer:** This is a **technical demo** for learning and experimentation. It is **not** a medical device and must **not** be used for diagnosis or treatment. Always consult a licensed clinician.

---

## 2) Key features (A → Z)

-  ✅**A**udio input via microphone (PortAudio/PyAudio).  
-  ✅**B**ase64 image encoding for safe multimodal requests.  
-  ✅**C**hat‑style reasoning over **text + image** with a Groq‑hosted LLM.  
-  ✅**D**evice‑agnostic web UI using **Gradio**.  
-  ✅**E**levenLabs TTS for natural doctor‑like voice; **gTTS** as a fallback.  
-  ✅**F**Fmpeg utilities for audio processing.  
-  ✅**G**roq **Whisper** ASR for robust transcription (`whisper-large-v3-turbo`).  
-  ✅**H**ardening suggestions: .env secrets, input size checks, PII redaction.  
-  ✅**I**mage understanding through a **vision‑capable** chat model (multimodal).  
-  ✅**J**ust‑works local run: `python gradio_app.py`.  
-  ✅**K**ey‑based config via `.env` (`GROQ_API_KEY`, `ELEVENLABS_API_KEY`).  
-  ✅**R**AG hooks present in code imports (future: plug a vector DB).  
-  ✅**T**esting stubs and portability (pip/conda/pipenv).

---

---

## 3) How it works (Pipeline)

```
[Mic Input] --(wav/mp3)--> [ASR: Groq Whisper] --(text)--> 
     +--> [Optional Image Upload] --(base64)--> 
            [Multimodal LLM (Groq, Meta Llama family)] --(doctor reply)--> 
                [TTS: ElevenLabs/gTTS] --(audio)--> [Playback in Gradio]
```

Core modules (from your source code):

- `voice_of_the_patient.py`  
  - `record_audio(...)` – records mic audio and writes MP3 (uses `speech_recognition`, PortAudio, FFmpeg/pydub).  
  - `transcribe_with_groq(...)` – sends audio to **Groq Whisper `whisper-large-v3-turbo`** and returns text.

- `brain_of_the_doctor.py`  
  - `encode_image(path)` – base64 encodes uploaded image.  
  - `analyze_image_with_query(image_path, query, model=...)` – calls Groq **multimodal chat** with both the image and your question. Code string mentions **`"meta-llama/llama-4-maverick-17b-...-turbo"`**.

- `voice_of_the_doctor.py`  
  - `text_to_speech_with_elevenlabs(text, outpath)` – streams high‑quality TTS via **ElevenLabs**.  
  - `text_to_speech_with_gtts(text, outpath)` – simple **gTTS** fallback (no API key needed).

- `gradio_app.py`  
  - Defines a **Gradio Interface** with:
    - Inputs: microphone **recording** button + **image** upload.  
    - Outputs: **transcript text**, **doctor reply**, **audio response**.  
  - Calls the three modules above to wire the full round‑trip.

---
