# Multimodal AI Healthcare Assistant

> Voice + Vision + LLM — a demo assistant that understands **speech**, **medical images**, and **text**, then replies with **doctor‑style spoken answers**.

---

## 1) What this project is ?

An end‑to‑end, **patient–doctor interaction prototype** built with **Gradio**. It lets a user:

- **Speak a symptom/question** → the app records your voice and transcribes it using **Groq Whisper (`whisper-large-v3-turbo`)**.  
- **Upload a clinical image** (e.g., a skin lesion scan, X‑ray, etc.) → the app encodes it and sends it along with your query to a **multimodal LLM** running on **Groq**.  
- **Receive a doctor's reply** → generated by a **Meta Llama family** model (code references a `"meta-llama/llama-4-maverick-17b-...-turbo"` string), and then converted to **speech** using **ElevenLabs** (fallback: **gTTS**).

> ⚠️ **Medical disclaimer:** This is a **technical demo** for learning and experimentation. It is **not** a medical device and must **not** be used for diagnosis or treatment. Always consult a licensed clinician.

---

## 2) Key features (A → Z)

-  ✅**A**udio input via microphone (PortAudio/PyAudio).  
-  ✅**B**ase64 image encoding for safe multimodal requests.  
-  ✅**C**hat‑style reasoning over **text + image** with a Groq‑hosted LLM.  
-  ✅**D**evice‑agnostic web UI using **Gradio**.  
-  ✅**E**levenLabs TTS for natural doctor‑like voice; **gTTS** as a fallback.  
-  ✅**F**Fmpeg utilities for audio processing.  
-  ✅**G**roq **Whisper** ASR for robust transcription (`whisper-large-v3-turbo`).  
-  ✅**H**ardening suggestions: .env secrets, input size checks, PII redaction.  
-  ✅**I**mage understanding through a **vision‑capable** chat model (multimodal).  
-  ✅**J**ust‑works local run: `python gradio_app.py`.  
-  ✅**K**ey‑based config via `.env` (`GROQ_API_KEY`, `ELEVENLABS_API_KEY`).  
-  ✅**R**AG hooks present in code imports (future: plug a vector DB).  
-  ✅**T**esting stubs and portability (pip/conda/pipenv).

---

---

## 3) How it works (Pipeline)

```
[Mic Input] --(wav/mp3)--> [ASR: Groq Whisper] --(text)--> 
     +--> [Optional Image Upload] --(base64)--> 
            [Multimodal LLM (Groq, Meta Llama family)] --(doctor reply)--> 
                [TTS: ElevenLabs/gTTS] --(audio)--> [Playback in Gradio]
```

Core modules (from your source code):

- `voice_of_the_patient.py`  
  - `record_audio(...)` – records mic audio and writes MP3 (uses `speech_recognition`, PortAudio, FFmpeg/pydub).  
  - `transcribe_with_groq(...)` – sends audio to **Groq Whisper `whisper-large-v3-turbo`** and returns text.

- `brain_of_the_doctor.py`  
  - `encode_image(path)` – base64 encodes uploaded image.  
  - `analyze_image_with_query(image_path, query, model=...)` – calls Groq **multimodal chat** with both the image and your question. Code string mentions **`"meta-llama/llama-4-maverick-17b-...-turbo"`**.

- `voice_of_the_doctor.py`  
  - `text_to_speech_with_elevenlabs(text, outpath)` – streams high‑quality TTS via **ElevenLabs**.  
  - `text_to_speech_with_gtts(text, outpath)` – simple **gTTS** fallback (no API key needed).

- `gradio_app.py`  
  - Defines a **Gradio Interface** with:
    - Inputs: microphone **recording** button + **image** upload.  
    - Outputs: **transcript text**, **doctor reply**, **audio response**.  
  - Calls the three modules above to wire the full round‑trip.

---

---
## 4) Project structure (excerpt)

```
Multimodal-AI-Healthcare-Assistant-main/
├── gradio_app.py
├── brain_of_the_doctor.py
├── voice_of_the_patient.py
├── voice_of_the_doctor.py
├── requirements.txt
├── Pipfile
├── .env               # holds GROQ_API_KEY / ELEVENLABS_API_KEY (do NOT commit real keys)
└── ...                # assets, notebooks, helpers (see repo for full tree)
```

> Auto‑detected: **Gradio** app present; model artifacts folder not committed. Requirements pin **gradio**, **fastapi**, **groq**, **elevenlabs**, **speechrecognition**, **uvicorn**, etc.

---

---
## 5) Setup

### Prereqs
- **Python**: recommended **3.13** (per `Pipfile`), 3.10+ also works for Gradio.  
- **FFmpeg** and **PortAudio** installed on your system.  
- Microphone permission enabled.

### Install (choose one)

**A) Pip (recommended)**
```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

pip install -r requirements.txt
```

**B) Pipenv**
```bash
pip install pipenv
pipenv install
pipenv shell
```

**C) Conda**
```bash
conda create -n multimodal-doc python=3.11 -y
conda activate multimodal-doc
pip install -r requirements.txt
```

### Environment variables
Create a `.env` in the project root:
```
GROQ_API_KEY=your_groq_key_here
ELEVENLABS_API_KEY=your_elevenlabs_key_here   # optional; if omitted, falls back to gTTS
```

---

## 6) Run

```bash
python gradio_app.py
# then open the local URL that Gradio prints (e.g., http://127.0.0.1:7860)
```

**What to try:**
1. Click **Record** and ask a medical‑style question (e.g., “I have a sore throat and fever for 3 days, what could it be?”).  
2. (Optional) Upload a relevant **image** (e.g., skin lesion).  
3. Receive: transcript → doctor reply (text) → **audio** playback.

---


---


## 7) Configuration & Customization

- **Switch TTS provider**: set `ELEVENLABS_API_KEY` to enable premium voice; remove it to use gTTS fallback.  
- **Change voices**: replace the ElevenLabs `voice_id` in `voice_of_the_doctor.py`.  
- **Model choice**: in `brain_of_the_doctor.py`, adjust the `model=` string to pick another **Groq** multimodal model.  
- **Rate‑limiting & timeouts**: add guards in the Gradio handlers for production robustness.  
- **RAG**: hook a vector DB (FAISS/Chroma) and augment prompts with guideline passages.

---

---
## 8) Results & Quality Notes

This app is **interactive** rather than a fixed benchmark. For meaningful evaluation, log test sessions and report:

- **ASR quality** (WER/CER) across accents/noise.  
- **Clinical helpfulness** via curated prompts (Likert scoring by SMEs).  
- **Hallucination** rate and refusal behavior for out‑of‑scope queries.  
- **Latency** (ASR → LLM → TTS round‑trip).

 
---

---
## 9) Real‑life applications (non‑diagnostic)

- **Triage assistant** for patient education before a clinic visit.  
- **Documentation helper** to summarize patient concerns in simple language.  
- **Training/teaching** tool to discuss differentials given a symptom + image.  
- **Accessibility** aid for users who prefer speaking & listening over typing/reading.

> Always place a **medical disclaimer** in the UI and require human review.

---

---
## 10) Troubleshooting

- **FFmpeg/PortAudio missing** → install them system‑wide; on Windows ensure `ffmpeg/bin` is in `%PATH%`.  
- **Mic not detected** → check OS permissions and default input device.  
- **No speech output** → verify `ELEVENLABS_API_KEY`; otherwise fallback to gTTS (internet required).  
- **Groq errors** → ensure `GROQ_API_KEY` is valid; check model name and rate limits.  
- **Unicode on Windows** → run in UTF‑8 console (`chcp 65001`).

---

---
## 11) Tech stack

- UI: **Gradio**  
- ASR: **Groq Whisper (`whisper-large-v3-turbo`)**  
- LLM (multimodal): **Meta Llama family** via **Groq** (code references a `"llama-4-maverick-17b-...-turbo"` string)  
- TTS: **ElevenLabs** (primary), **gTTS** (fallback)  
- Utilities: **pydub**, **ffmpeg**, **speech_recognition**, **uvicorn/fastapi** available for API extension

---

---

## 12) Security & Compliance

- Store keys only in `.env` / secret managers. **Never commit real keys.**  
- If handling clinical data/images, ensure **de‑identification**, consent, and compliance with local laws.  
- Add a log redactor; avoid storing raw audio/images in production.

---

---

## 13) License & Credits

- License: see repository (no explicit license file detected at generation time).  
- Credits: Groq, ElevenLabs, Gradio, and the broader open‑source community.

---
